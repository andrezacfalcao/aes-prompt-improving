"""Exploratory analysis utilities for ASAP splits.

This script loads processed splits generated by ``scripts/prepare_data.py`` and
produces lightweight exploratory artefacts:

* Histogramas das distribuições de scores por prompt/split
* Estatísticas descritivas agregadas
* Curvas simples de aprendizado (loss/metric por época) extraídas dos logs JSON
* Tabelas de erros (diferença absoluta previsão vs. score real)

Resultados são salvos em ``analysis/`` para consulta rápida.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns


PROMPT_IDS = tuple(range(1, 9))
SPLITS = ("train", "val", "test")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ASAP exploratory analysis")

    parser.add_argument(
        "--processed_dir",
        type=str,
        default="data/processed",
        help="Directory containing processed prompt splits"
    )

    parser.add_argument(
        "--format",
        choices=("csv", "parquet"),
        default=None,
        help="Explicit split format (auto-detected otherwise)"
    )

    parser.add_argument(
        "--output_dir",
        type=str,
        default="analysis",
        help="Directory to store generated artefacts"
    )

    parser.add_argument(
        "--include_prompts",
        type=int,
        nargs="*",
        default=list(PROMPT_IDS),
        help="Subset of prompts to analyse (default: all)"
    )

    parser.add_argument(
        "--results_dir",
        type=str,
        default="checkpoints",
        help="Root directory containing experiment results JSON files"
    )

    parser.add_argument(
        "--metrics",
        type=str,
        nargs="*",
        default=("qwk", "pearson", "spearman", "rmse", "mae"),
        help="Metrics to plot from training logs when available"
    )

    parser.add_argument(
        "--show_plots",
        action="store_true",
        help="Display plots interactively in addition to saving"
    )

    return parser.parse_args()


def load_metadata(processed_dir: Path) -> Dict:
    metadata_path = processed_dir / "metadata.json"
    if metadata_path.exists():
        with open(metadata_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def infer_format(processed_dir: Path, explicit: Optional[str]) -> str:
    if explicit:
        return explicit
    metadata = load_metadata(processed_dir)
    fmt = metadata.get("format")
    if fmt in {"csv", "parquet"}:
        return fmt
    raise ValueError(
        "Unable to infer processed split format. Provide --format explicitly."
    )


def load_split_frame(processed_dir: Path, prompt: int, split: str, fmt: str) -> pd.DataFrame:
    split_path = processed_dir / f"prompt_{prompt}" / f"{split}.{fmt}"
    if not split_path.exists():
        raise FileNotFoundError(f"Split not found: {split_path}")
    if fmt == "csv":
        return pd.read_csv(split_path)
    if fmt == "parquet":
        return pd.read_parquet(split_path)
    raise ValueError(f"Unsupported format: {fmt}")


def ensure_output_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def describe_scores(df: pd.DataFrame) -> pd.DataFrame:
    desc = df["domain1_score"].describe().to_frame().T
    desc["count"] = len(df)
    return desc


def plot_score_distribution(df: pd.DataFrame, prompt: int, split: str, output_dir: Path, show: bool) -> None:
    ensure_output_dir(output_dir)
    plt.figure(figsize=(8, 4))
    sns.histplot(df["domain1_score"], bins=20, kde=True)
    plt.title(f"Prompt {prompt} | {split} score distribution")
    plt.xlabel("Score")
    plt.ylabel("Frequency")
    plot_path = output_dir / f"prompt_{prompt}_{split}_hist.png"
    plt.tight_layout()
    plt.savefig(plot_path)
    if show:
        plt.show()
    plt.close()


def aggregate_statistics(all_stats: Dict[int, Dict[str, pd.DataFrame]]) -> pd.DataFrame:
    records = []
    for prompt, split_stats in all_stats.items():
        for split, stats_df in split_stats.items():
            stats = stats_df.squeeze()
            record = {
                "prompt": prompt,
                "split": split,
                "count": stats.get("count", np.nan),
                "mean": stats.get("mean", np.nan),
                "std": stats.get("std", np.nan),
                "min": stats.get("min", np.nan),
                "max": stats.get("max", np.nan)
            }
            records.append(record)
    return pd.DataFrame.from_records(records)


def find_result_files(results_dir: Path, prompt: int) -> List[Path]:
    pattern = f"*p{prompt}*/results.json"
    return list(results_dir.glob(pattern))


def load_training_history(result_path: Path) -> Optional[pd.DataFrame]:
    history_path = result_path.parent / "history.json"
    if not history_path.exists():
        return None
    with open(history_path, "r", encoding="utf-8") as f:
        history = json.load(f)
    return pd.DataFrame(history)


def plot_training_curves(history: pd.DataFrame, metrics: List[str], prompt: int, output_dir: Path, show: bool) -> None:
    ensure_output_dir(output_dir)
    fig, axes = plt.subplots(len(metrics), 1, figsize=(8, 4 * len(metrics)))
    if len(metrics) == 1:
        axes = [axes]

    epochs = history.get("epoch", list(range(len(history))))

    for ax, metric in zip(axes, metrics):
        if metric not in history:
            ax.set_visible(False)
            continue
        ax.plot(epochs, history[metric], marker="o")
        ax.set_title(f"Prompt {prompt} | {metric}")
        ax.set_xlabel("Epoch")
        ax.set_ylabel(metric)

    plt.tight_layout()
    plot_path = output_dir / f"prompt_{prompt}_training_curves.png"
    plt.savefig(plot_path)
    if show:
        plt.show()
    plt.close(fig)


def compute_error_table(df: pd.DataFrame, prompt: int, output_dir: Path) -> None:
    if {"prediction", "domain1_score"}.issubset(df.columns):
        df = df.copy()
        df["abs_error"] = (df["prediction"] - df["domain1_score"]).abs()
        summary = df.sort_values("abs_error", ascending=False).head(25)
        ensure_output_dir(output_dir)
        summary.to_csv(output_dir / f"prompt_{prompt}_top_errors.csv", index=False)


def load_error_details(result_path: Path) -> Optional[pd.DataFrame]:
    error_path = result_path.parent / "errors.csv"
    if not error_path.exists():
        return None
    return pd.read_csv(error_path)


def main():
    args = parse_args()

    processed_dir = Path(args.processed_dir)
    output_dir = Path(args.output_dir)
    results_dir = Path(args.results_dir)

    ensure_output_dir(output_dir)

    try:
        fmt = infer_format(processed_dir, args.format)
    except ValueError as exc:
        print(f"Error: {exc}")
        return

    prompts = sorted({p for p in args.include_prompts if p in PROMPT_IDS})
    if not prompts:
        print("No valid prompts selected; exiting.")
        return

    all_stats: Dict[int, Dict[str, pd.DataFrame]] = {}

    for prompt in prompts:
        prompt_stats: Dict[str, pd.DataFrame] = {}
        for split in SPLITS:
            try:
                df = load_split_frame(processed_dir, prompt, split, fmt)
            except FileNotFoundError:
                print(f"Skipping missing split {split} for prompt {prompt}")
                continue

            stats = describe_scores(df)
            prompt_stats[split] = stats

            plot_dir = output_dir / "distributions"
            plot_score_distribution(df, prompt, split, plot_dir, args.show_plots)

            if split == "test":
                result_files = find_result_files(results_dir, prompt)
                for result_path in result_files:
                    errors_df = load_error_details(result_path)
                    if errors_df is not None:
                        compute_error_table(errors_df, prompt, output_dir / "errors")

        if prompt_stats:
            all_stats[prompt] = prompt_stats

    if all_stats:
        summary_df = aggregate_statistics(all_stats)
        summary_path = output_dir / "score_statistics.csv"
        summary_df.to_csv(summary_path, index=False)
        print(f"Saved score statistics to {summary_path}")

    # Training curves
    for prompt in prompts:
        result_files = find_result_files(results_dir, prompt)
        for result_path in result_files:
            history_df = load_training_history(result_path)
            if history_df is not None:
                plot_training_curves(
                    history_df,
                    list(args.metrics),
                    prompt,
                    output_dir / "training_curves",
                    args.show_plots
                )

    print("Analysis artefacts generated under", output_dir)


if __name__ == "__main__":
    main()


