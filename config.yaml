# ============================================
# Baseline & Future Techniques Configuration
# ============================================
# Consolidated configuration covering:
# - Baseline transformer experiments (BERT, RoBERTa, DistilBERT, ELECTRA)
# - Planned PANN (Prompt-Aware Neural Network)
# - Planned DRL (Deep Reinforcement Learning) extensions
# ============================================

# Model Architecture Configuration
model:
  baseline:
    name: "transformer_aes"
    backbones:
      - "bert-base-uncased"
      - "roberta-base"
      - "distilbert-base-uncased"
      - "google/electra-base-discriminator"
    dropout: 0.1
    hidden_size: null
    freeze_encoder: false

  pann:
    enabled: false
    name: "PANN"
    bert_model: "bert-base-uncased"
    bert_hidden_size: 768
    freeze_bert: false
    num_kernels: 8
    kernel_width: 0.1
    embedding_dim: 768
    fc_layers: 2
    fc_hidden_size: 256
    dropout: 0.1
    activation: "relu"
    max_essay_length: 512
    max_prompt_length: 128

  drl:
    enabled: false
    cnaa:
      delta_h: 0.8
      delta_l: 0.3
      margin_m1: 0.1
      margin_m2: 0.1
      score_reduction_mu: 0.4
      score_reduction_sigma: 1.0
      augmentation_ratio: 1.0
      nia_weight: 1.0
      asa_weight: 1.0
      pretrain_epochs: 5
      pretrain_learning_rate: 5e-5

    cst:
      alpha: 0.8
      prompt_replace_ratio_random: 0.5
      replace_ratios: [0.2, 0.3, 0.5]
      score_multipliers: [1.1, 1.0, 0.9]
      warmup_epochs: 2
      warmup_learning_rate: 5e-5
      finetune_epochs: 3
      finetune_learning_rate: 3e-5
      cf_data_ratio: 1.0

training:
  batch_size: 8
  num_epochs: 10
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4
  optimizer: "adamw"
  scheduler: "linear"
  max_length: 512
  num_workers: 0
  pin_memory: false
  fp16: false
  early_stopping:
    enabled: true
    patience: 3
    min_delta: 0.001
cnaa:
  delta_h: 0.8
  delta_l: 0.3

  use_natural_division: false

  margin_m1: 0.1
  margin_m2: 0.1

  score_reduction_mu: 0.4
  score_reduction_sigma: 1.0

  augmentation_ratio: 1.0
  nia_weight: 1.0
  asa_weight: 1.0

  pretrain_epochs: 5
  pretrain_learning_rate: 5e-5

cst:
  alpha: 0.8

  prompt_replace_ratio_random: 0.5
  replace_ratios: [0.2, 0.3, 0.5]
  score_multipliers: [1.1, 1.0, 0.9]

  warmup_epochs: 2
  warmup_learning_rate: 5e-5

  finetune_epochs: 3
  finetune_learning_rate: 3e-5

  cf_data_ratio: 1.0

data:
  dataset: "asap"
  raw_path: "data/raw/training_set_rel3.tsv"
  processed_dir: "data/processed"
  processed_format: "parquet"
  val_size: 0.1
  random_seed: 42
  use_cache: true
  cache_dir: "data/cache/"

evaluation:
  primary_metric: "qwk"
  additional_metrics: ["pearson", "spearman", "mse", "mae"]

  eval_during_training: true
  eval_on_validation: true
  eval_batch_size: 64

  num_runs: 5
  seeds: [42, 123, 456, 789, 1024]

  rescale_scores: true

experiment:
  name: "baseline_leave_one_out"
  output_dir: "checkpoints"
  log_dir: "logs"
  summary_dir: "analysis"
  prompts: [1, 2, 3, 4, 5, 6, 7, 8]
  models: ["bert-base-uncased", "roberta-base", "distilbert-base-uncased", "google/electra-base-discriminator"]
  skip_completed: true
  use_processed_splits: true
  processed_format: "parquet"

visualization:
  generate_tsne: true
  tsne_perplexity: 30
  tsne_n_iter: 1000

  plot_features: true
  plot_frequency: 1

  save_plots: true
  plot_format: "png"
  plot_dpi: 300

logging:
  level: "INFO"
  log_to_file: true
  log_file: "logs/training.log"

  log_to_console: true

  use_tensorboard: true
  tensorboard_dir: "runs/"

  use_wandb: false
  wandb_project: "prompt-aware-aes"
  wandb_entity: null

hardware:
  cuda_device: 0
  use_amp: false
  use_ddp: false
  world_size: 1

  gradient_accumulation_steps: 1
  gradient_checkpointing: false

reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

advanced:
  debug: false
  debug_samples: 200
  profile: false
  profile_steps: 100
  local_rank: -1
  notes:
    - "PANN/DRL sections disabled by default; flip 'enabled' flags when ready."
    - "Baseline experiments controlled via scripts/train.py and run_all_prompts.py."
